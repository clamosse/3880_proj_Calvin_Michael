---
title: "projectCode"
output: word_document
date: "2024-04-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tree)
library(randomForest)
library(glmnet)
library(dplyr)
library(gbm)
```



import and split data
```{r}
original_data <- read.csv("data/hermit_lake_weather_and_snow.csv",header=T)

original_data_SNWD_change <- subset(original_data, select = -SNWD_change_positive)
original_data_SNWD_change_positive <- subset(original_data, select = -SNWD_change)

train_size <- floor(0.8 * nrow(original_data))

set.seed(1)
train_indices <- sample(seq_len(nrow(original_data)), size = train_size, replace = FALSE)

train_data <- original_data_SNWD_change[train_indices, ]
test_data <- original_data_SNWD_change[-train_indices, ]

train_data_dichotomized  <- original_data_SNWD_change_positive[train_indices, ]
test_data_dichotomized  <- original_data_SNWD_change_positive[-train_indices, ]


```


lm model
```{r}
lm_model <- lm(data = train_data, SNWD_change ~.)
summary(lm_model)

predictions <- predict(lm_model, newdata = test_data)
lm_model_mse <- mean((test_data$SNWD_change - predictions)^2)
lm_model_mse
```

decision tree model
```{r}
tree_model <- tree(data = train_data, SNWD_change ~.)

plot(tree_model)
text(tree_model,cex=.5)

predictions <- predict(tree_model, newdata = test_data)
tree_model_mse <- mean((test_data$SNWD_change - predictions)^2)
tree_model_mse
```

pruning the tree
```{r}
cv_tree <- cv.tree(tree_model)
plot(cv_tree$size, cv_tree$dev, type = "b", xlab = "Tree Size", ylab = "CV Deviance")
optimal_tree_size <- which.min(cv_tree$dev)

tree_prunded <- prune.tree(tree_model, best = optimal_tree_size)
plot(tree_prunded)
text(tree_prunded,cex=.5)

pruned_predictions <- predict(tree_prunded, newdata = test_data)

pruned_mse <- mean((test_data$SNWD_change - pruned_predictions)^2)
pruned_mse
```

bagged model
```{r}
bagged_model <- randomForest(data = train_data, SNWD_change ~., mtry = 27, ntree = 1000, importance = T)
bagged_predictions <- predict(bagged_model, newdata = test_data)
bagged_mse <- mean((test_data$SNWD_change - bagged_predictions)^2)
bagged_mse

importance(bagged_model)
varImpPlot(bagged_model)
```

Boosting
```{r}
boost_model <- gbm(SNWD_change ~., data = train_data, distribution = "gaussian")

boost <- gbm(mpg ~ ., data = train,
             distribution = "gaussian",
             n.trees = 1000, shrinkage = 0.01,
             interaction.depth = 4,
             bag.fraction = 0.7,
             n.minobsinnode = 5)


```

random forest
```{r}
mse_values <- NULL

for (i in 1:27) {
  rf_model <- randomForest(SNWD_change ~ ., data = train_data, mtry = i, ntree = 500)
  rf_predictions <- predict(rf_model, newdata = test_data)
  mse_values[i] <- mean((test_data$SNWD_change - rf_predictions)^2)
}

mse_values

position <- seq_along(mse_values)
plot(position, mse_values, type = "o", 
     xlab = "Position in the List", ylab = "MSE Values", 
     main = "Plot of MSE Values vs. mtry")


rf_model <- randomForest(SNWD_change ~ ., data = train_data, mtry = 7, ntree = 1000, importance = T)
rf_predictions <- predict(rf_model, newdata = test_data)

rf_mse <- mean((test_data$SNWD_change - rf_predictions)^2)
rf_mse

importance(rf_model)
varImpPlot(rf_model)
```

ridge Regression
```{r}
subset_data <- test_data[, !(names(test_data) == "SNWD_change")]
scaled_data <- scale(subset_data)
test_matrix <- as.matrix(scaled_data)

subset_data <- train_data[, !(names(train_data) == "SNWD_change")]
scaled_data <- scale(subset_data)
train_matrix <- as.matrix(scaled_data)


feature_names <- colnames(subset_data)
feature_names <- c('intercept', feature_names)

#using cv to find best value of lambda
cv.ridge <- cv.glmnet(train_matrix,train_data$SNWD_change, alpha = 0)  # alpha = 1 for Ridge
plot(cv.ridge)
best_lambda <- cv.ridge$lambda.min

#best lasso model
best_ridge <- glmnet(train_matrix,train_data$SNWD_change, alpha = 0, lambda = best_lambda)


#best lasso model coefficients
ridge.coef <- coef(best_ridge)
ridge.coef <- data.frame(ridge.coef@Dimnames[[1]][ridge.coef@i+1],ridge.coef@x)
names(ridge.coef) <- c('var','val')
ridge.coef <- ridge.coef %>% arrange(-abs(val)) %>% print(.,n=25)

  #finding mse
ridge_predictions <- predict(best_ridge, newx = test_matrix)
ridge_mse <- mean((test_data$SNWD_change - ridge_predictions)^2)       
ridge_mse

#finding r2
sst <- sum((test_data$SNWD_change - mean(test_data$SNWD_change))^2)
sse <- sum((ridge_predictions - test_data$SNWD_change)^2)
ridge_r2 <- 1 - sse/sst
ridge_r2
```

Lasso Regression
```{r}
#using cv to find best value of lambda
cv.lasso <- cv.glmnet(train_matrix,train_data$SNWD_change, alpha = 1)  # alpha = 1 for Lasso
plot(cv.lasso)
best_lambda <- cv.lasso$lambda.min

#best lasso model
best_lasso <- glmnet(train_matrix,train_data$SNWD_change, alpha = 1, lambda = best_lambda)

#best lasso model coefficients
lasso.coef <- coef(best_lasso)
lasso.coef <- data.frame(lasso.coef@Dimnames[[1]][lasso.coef@i+1],lasso.coef@x)
names(lasso.coef) <- c('var','val')
lasso.coef <- lasso.coef %>% arrange(-abs(val)) %>% print(.,n=25)

#finding mse
lasso_predictions <- predict(best_lasso, newx = test_matrix)
lasso_mse <- mean((test_data$SNWD_change - lasso_predictions)^2)       
lasso_mse

#finding r2
sst <- sum((test_data$SNWD_change - mean(test_data$SNWD_change))^2)
sse <- sum((lasso_predictions - test_data$SNWD_change)^2)
lasso_r2 <- 1 - sse/sst
lasso_r2

```

<<<<<<< HEAD



classification models

log reg model
```{r}
full_logReg_model <- glm(SNWD_change_positive ~.,data = train_data_dichotomized, family = 'binomial')
summary(full_logReg_model)

predictions <- predict(full_logReg_model, type = "response", newdata = test_data_dichotomized)

threshold <- 0.5
predicted_class <- ifelse(predictions > threshold, 1, 0)
full_log_reg_confusion_matrix <- table(predicted_class, test_data_dichotomized$SNWD_change_positive)

correct_pred <- sum(diag(full_log_reg_confusion_matrix))
total_pred <- sum(full_log_reg_confusion_matrix)

full_log_reg_accuracy <- correct_pred / total_pred


full_log_reg_confusion_matrix
full_log_reg_accuracy



pruned_logReg_model <- glm(SNWD_change_positive ~ SNOW + snow,data = train_data_dichotomized, family = 'binomial')
summary(pruned_logReg_model)

predictions <- predict(pruned_logReg_model, type = "response", newdata = test_data_dichotomized)

threshold <- 0.5
predicted_class <- ifelse(predictions > threshold, 1, 0)
log_reg_confusion_matrix <- table(predicted_class, test_data_dichotomized$SNWD_change_positive)

correct_pred <- sum(diag(log_reg_confusion_matrix))
total_pred <- sum(log_reg_confusion_matrix)

log_reg_accuracy <- correct_pred / total_pred


log_reg_confusion_matrix
log_reg_accuracy

```

=======
MSE for all the models
```{r}

mse_values <- c(lm_model_mse, tree_model_mse, pruned_mse, bagged_mse, rf_mse, ridge_mse, lasso_mse)

model_names <- c("Linear Regression", "Decision Tree", "Pruned Decision Tree", "Random Forest", "Random Forest (mtry=7)", "Ridge Regression", "Lasso Regression")
# Create a wider plot area
par(mar = c(10, 5, 4, 2) + 0.1)  # Adjust the margin to make room for longer labels

# Add text labels with the MSE values on top of each bar
text(x = barplot(mse_values, names.arg = model_names, main = "Mean Squared Error (MSE) of Different Models", ylab = "MSE", col = "steelblue", las = 2), y = mse_values, labels = round(mse_values, 2), pos = 3, cex = .8)
```
>>>>>>> cfab1a9a06f3604c2fa90c1e3768e9ef9286ccc0
